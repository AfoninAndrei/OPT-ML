{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "OPT_ML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0117p6anAfu"
      },
      "source": [
        "# Includes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7hoPiykkxNI"
      },
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import multiprocessing\n",
        "import os.path\n",
        "import csv\n",
        "import copy\n",
        "import joblib\n",
        "from torchvision import datasets\n",
        "import torchvision\n",
        "import seaborn as sns; sns.set(color_codes=True)\n",
        "sns.set_style(\"white\")\n",
        "from pdb import set_trace as bp\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXJn7T_vl3nv"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "def w(v):\n",
        "    if USE_CUDA:\n",
        "        return v.cuda()\n",
        "    return v"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BHrPoBBmYrv"
      },
      "source": [
        "from meta_module import *"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxUmA1F8mtQS"
      },
      "source": [
        "!mkdir -p _cache\n",
        "cache = joblib.Memory(location='_cache', verbose=0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnM1B_9usXQs"
      },
      "source": [
        "# Hamiltonian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RusgpHNMsiTm"
      },
      "source": [
        "! pip install git+https://github.com/rtqichen/torchdiffeq\n",
        "from torchdiffeq import odeint_adjoint as odeint\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI1gYuSPsWXI"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchdiffeq\n",
        "\n",
        "class ODEBlock(nn.Module):\n",
        "    def __init__(self, odefunc:nn.Module, method:str='dopri5', rtol:float=1e-3, atol:float=1e-4, adjoint:bool=True):\n",
        "        \"\"\" Standard ODEBlock class. Can handle all types of ODE functions\n",
        "            :method:str = {'euler', 'rk4', 'dopri5', 'adams', 'scipy_solver'}\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.odefunc = odefunc\n",
        "        self.method = method\n",
        "        self.adjoint_flag = adjoint\n",
        "        self.atol, self.rtol = atol, rtol\n",
        "\n",
        "    def forward(self, x:torch.Tensor, T:int=1):\n",
        "        self.integration_time = torch.tensor([0, T]).float()\n",
        "        self.integration_time = self.integration_time.type_as(x)\n",
        "\n",
        "        if self.adjoint_flag:\n",
        "            out = torchdiffeq.odeint_adjoint(self.odefunc, x, self.integration_time,\n",
        "                                             rtol=self.rtol, atol=self.atol, method=self.method)\n",
        "        else:\n",
        "            out = torchdiffeq.odeint(self.odefunc, x, self.integration_time,\n",
        "                                     rtol=self.rtol, atol=self.atol, method=self.method)\n",
        "        # print(out.shape)\n",
        "        return out[-1]\n",
        "    \n",
        "    def forward_batched(self, x:torch.Tensor, nn:int, indices:list, timestamps:set):\n",
        "        \"\"\" Modified forward for ODE batches with different integration times \"\"\"\n",
        "        timestamps = torch.Tensor(list(timestamps))\n",
        "        if self.adjoint_flag:\n",
        "            out = torchdiffeq.odeint_adjoint(self.odefunc, x, timestamps,\n",
        "                                             rtol=self.rtol, atol=self.atol, method=self.method)\n",
        "        else:\n",
        "            out = torchdiffeq.odeint(self.odefunc, x, timestamps,\n",
        "                                     rtol=self.rtol, atol=self.atol, method=self.method)\n",
        "\n",
        "        out = self._build_batch(out, nn, indices).reshape(x.shape)\n",
        "        return out\n",
        "    \n",
        "    def _build_batch(self, odeout, nn, indices):\n",
        "        b_out = []\n",
        "        for i in range(len(indices)):\n",
        "            b_out.append(odeout[indices[i],i*nn:(i+1)*nn])\n",
        "        return torch.cat(b_out).to(odeout.device)\n",
        "              \n",
        "        \n",
        "    def trajectory(self, x:torch.Tensor, T:int, num_points:int):\n",
        "        self.integration_time = torch.linspace(0, T, num_points)\n",
        "        self.integration_time = self.integration_time.type_as(x)\n",
        "        out = torchdiffeq.odeint(self.odefunc, x, self.integration_time,\n",
        "                                 rtol=self.rtol, atol=self.atol, method=self.method)\n",
        "        return out"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf88pdoyosnX"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "class Hamiltonian(nn.Module):\n",
        "    def __init__(self, input_dim:int):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        #self.g = nn.Parameter(torch.rand((int(input_dim)), dtype=torch.float32, requires_grad=True))\n",
        "        #self.v = nn.Parameter(torch.rand((1), dtype=torch.float32, requires_grad=True))\n",
        "        self.g = nn.Sequential(nn.Linear(int(input_dim), int(input_dim)), nn.ReLU(), nn.Linear(int(input_dim), 1))\n",
        "        self.v = nn.Sequential(nn.Linear(int(input_dim), int(input_dim)), nn.ReLU(), nn.Linear(int(input_dim), 1))\n",
        "        self.D = nn.Linear(2*input_dim, 2*input_dim)\n",
        "        self.L = nn.Linear(int(input_dim), int(input_dim))\n",
        "        self.L.weight.data.fill_(0.0)\n",
        "        self.L.weight.data += torch.eye((int(self.input_dim)), dtype=torch.float32, requires_grad=True)\n",
        "        self.nfe = 0\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        with torch.enable_grad():\n",
        "            one = torch.tensor(1, dtype=torch.float32, device=self.device, requires_grad=True)\n",
        "            x = one * x\n",
        "            self.nfe += 1\n",
        "            q, dev_q = torch.chunk(x, 2, dim=-1)\n",
        "            #print(q)\n",
        "            g = self.g(q)\n",
        "            H = self.v(q) + torch.sum(torch.pow(torch.matmul(dev_q, self.L.weight.t()), 2)) / 2.0 \n",
        "            H = H.reshape(-1,)\n",
        "            M = torch.matmul(self.L.weight, self.L.weight.t()) + 1e-12*torch.eye((int(self.input_dim)), dtype=torch.float32, device=self.device)\n",
        "            \n",
        "            dH_q = torch.autograd.grad(H, q, grad_outputs=torch.ones_like(H), create_graph=True)[0]\n",
        "            D_q, D_p = torch.chunk(self.D(torch.cat((dH_q, dev_q), dim=-1)), 2, dim=-1)\n",
        "            out = torch.cat((dev_q-D_q, torch.matmul(-dH_q-D_p+g, M.inverse())), dim=-1).view_as(x)\n",
        "        return out"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_cMa1uOR1l5"
      },
      "source": [
        "# class Hamiltonian(nn.Module):\n",
        "#     def __init__(self, input_dim:int):\n",
        "#         \"\"\"Standard GCDN ODE function class. To be passed to an ODEBlock\"\"\"\n",
        "#         super().__init__()\n",
        "#         self.lin1 = nn.Linear(2*input_dim, 20)\n",
        "#         self.act = nn.Tanh()\n",
        "#         self.lin2 = nn.Linear(20, 2)\n",
        "#         self.nfe = 0\n",
        "\n",
        "#     def forward(self, t, x):\n",
        "#         self.nfe += 1\n",
        "#         x = self.lin1(x)\n",
        "#         x = self.act(x)\n",
        "#         x = self.lin2(x)\n",
        "#         return x"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqKZY8Fjr8uv"
      },
      "source": [
        "# Funcs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWlVJJizmcaY"
      },
      "source": [
        "def detach_var(v):\n",
        "    var = w(Variable(v.data, requires_grad=True))\n",
        "    var.retain_grad()\n",
        "    return var\n",
        "\n",
        "import functools\n",
        "\n",
        "def rsetattr(obj, attr, val):\n",
        "    pre, _, post = attr.rpartition('.')\n",
        "    return setattr(rgetattr(obj, pre) if pre else obj, post, val)\n",
        "\n",
        "# using wonder's beautiful simplification: https://stackoverflow.com/questions/31174295/getattr-and-setattr-on-nested-objects/31174427?noredirect=1#comment86638618_31174427\n",
        "\n",
        "def rgetattr(obj, attr, *args):\n",
        "    def _getattr(obj, attr):\n",
        "        return getattr(obj, attr, *args)\n",
        "    return functools.reduce(_getattr, [obj] + attr.split('.'))\n",
        "\n",
        "def do_fit(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=True):\n",
        "    if should_train:\n",
        "        opt_net.train()\n",
        "    else:\n",
        "        opt_net.eval()\n",
        "        unroll = 1\n",
        "    \n",
        "    target = target_cls(training=should_train)\n",
        "    optimizee = w(target_to_opt())\n",
        "    n_params = 0\n",
        "    for name, p in optimizee.all_named_parameters():\n",
        "        n_params += int(np.prod(p.size()))\n",
        "    hidden_states = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
        "    cell_states = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
        "    all_losses_ever = []\n",
        "    if should_train:\n",
        "        meta_opt.zero_grad()\n",
        "    all_losses = None\n",
        "    for iteration in range(1, optim_it + 1):\n",
        "        loss = optimizee(target)\n",
        "                    \n",
        "        if all_losses is None:\n",
        "            all_losses = loss\n",
        "        else:\n",
        "            all_losses += loss\n",
        "        \n",
        "        all_losses_ever.append(loss.data.cpu().numpy())\n",
        "        loss.backward(retain_graph=should_train)\n",
        "\n",
        "        offset = 0\n",
        "        result_params = {}\n",
        "        hidden_states2 = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
        "        cell_states2 = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
        "        for name, p in optimizee.all_named_parameters():\n",
        "            cur_sz = int(np.prod(p.size()))\n",
        "            # We do this so the gradients are disconnected from the graph but we still get\n",
        "            # gradients from the rest\n",
        "            gradients = detach_var(p.grad.view(cur_sz, 1))\n",
        "            updates, new_hidden, new_cell = opt_net(\n",
        "                gradients,\n",
        "                [h[offset:offset+cur_sz] for h in hidden_states],\n",
        "                [c[offset:offset+cur_sz] for c in cell_states]\n",
        "            )\n",
        "            for i in range(len(new_hidden)):\n",
        "                hidden_states2[i][offset:offset+cur_sz] = new_hidden[i]\n",
        "                cell_states2[i][offset:offset+cur_sz] = new_cell[i]\n",
        "            result_params[name] = p + updates.view(*p.size()) * out_mul\n",
        "            result_params[name].retain_grad()\n",
        "            \n",
        "            offset += cur_sz\n",
        "            \n",
        "        if iteration % unroll == 0:\n",
        "            if should_train:\n",
        "                meta_opt.zero_grad()\n",
        "                all_losses.backward()\n",
        "                meta_opt.step()\n",
        "                \n",
        "            all_losses = None\n",
        "\n",
        "            optimizee = w(target_to_opt())\n",
        "            optimizee.load_state_dict(result_params)\n",
        "            optimizee.zero_grad()\n",
        "            hidden_states = [detach_var(v) for v in hidden_states2]\n",
        "            cell_states = [detach_var(v) for v in cell_states2]\n",
        "            \n",
        "        else:\n",
        "            for name, p in optimizee.all_named_parameters():\n",
        "                rsetattr(optimizee, name, result_params[name])\n",
        "            assert len(list(optimizee.all_named_parameters()))\n",
        "            hidden_states = hidden_states2\n",
        "            cell_states = cell_states2\n",
        "            \n",
        "    return all_losses_ever\n",
        "\n",
        "\n",
        "@cache.cache\n",
        "def fit_optimizer(target_cls, target_to_opt, preproc=False, unroll=20, optim_it=100, n_epochs=20, n_tests=100, lr=0.001, out_mul=1.0):\n",
        "    opt_net = w(Optimizer(preproc=preproc))\n",
        "    #opt_net = w(Optimizer_HNN(preproc=preproc))\n",
        "    meta_opt = optim.Adam(opt_net.parameters(), lr=lr, weight_decay=3e-6)\n",
        "    scheduler = StepLR(meta_opt, step_size=1, gamma=0.9)\n",
        "    best_net = None\n",
        "    best_loss = 100000000000000000\n",
        "    \n",
        "    for _ in tqdm(range(n_epochs), 'epochs'):\n",
        "        for _ in tqdm(range(20), 'iterations'):\n",
        "            do_fit(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=True)\n",
        "        \n",
        "        loss = (np.mean([\n",
        "            np.mean(do_fit(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=False))\n",
        "            for _ in tqdm(range(n_tests), 'tests')\n",
        "        ]))\n",
        "        print(loss)\n",
        "        if loss < best_loss:\n",
        "            print(best_loss, loss)\n",
        "            best_loss = loss\n",
        "            best_net = copy.deepcopy(opt_net.state_dict())\n",
        "        scheduler.step() \n",
        "    return best_loss, best_net"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxYGFcsVzl1T"
      },
      "source": [
        "def do_fit_HNN(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=True):\n",
        "    if should_train:\n",
        "        opt_net.train()\n",
        "    else:\n",
        "        opt_net.eval()\n",
        "        unroll = 1\n",
        "    \n",
        "    target = target_cls(training=should_train)\n",
        "    optimizee = w(target_to_opt())\n",
        "    n_params = 0\n",
        "    for name, p in optimizee.all_named_parameters():\n",
        "        n_params += int(np.prod(p.size()))\n",
        "    #hidden_states = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
        "    derivative_input = w(Variable(torch.zeros(n_params, 1)))\n",
        "    all_losses_ever = []\n",
        "    if should_train:\n",
        "        meta_opt.zero_grad()\n",
        "    all_losses = None\n",
        "    for iteration in range(1, optim_it + 1):\n",
        "        loss = optimizee(target)\n",
        "        # print(iteration)      \n",
        "        if all_losses is None:\n",
        "            all_losses = loss\n",
        "        else:\n",
        "            all_losses += loss\n",
        "        \n",
        "        all_losses_ever.append(loss.data.cpu().numpy())\n",
        "        loss.backward(retain_graph=should_train)\n",
        "\n",
        "        offset = 0\n",
        "        result_params = {}\n",
        "        derivative_input2 = w(Variable(torch.zeros(n_params, 1)))\n",
        "        for name, p in optimizee.all_named_parameters():\n",
        "            cur_sz = int(np.prod(p.size()))\n",
        "            # We do this so the gradients are disconnected from the graph but we still get\n",
        "            # gradients from the rest\n",
        "            gradients = detach_var(p.grad.view(cur_sz, 1))\n",
        "            # print(p.shape, gradients.shape)\n",
        "            inp = torch.cat((gradients, derivative_input[offset:offset+cur_sz]), dim=-1)\n",
        "            updates = opt_net(inp)\n",
        "            #derivative_input2[offset:offset+cur_sz, 0] = updates.view(*p.size()) * out_mul\n",
        "            derivative_input2[offset:offset+cur_sz] = updates.view(*gradients.size()) * out_mul\n",
        "            result_params[name] = p + updates.view(*p.size()) * out_mul\n",
        "            result_params[name].retain_grad()\n",
        "            offset += cur_sz\n",
        "            \n",
        "        if iteration % unroll == 0:\n",
        "            if should_train:\n",
        "                meta_opt.zero_grad()\n",
        "                all_losses.backward()\n",
        "                meta_opt.step()\n",
        "                \n",
        "            all_losses = None\n",
        "\n",
        "            optimizee = w(target_to_opt())\n",
        "            optimizee.load_state_dict(result_params)\n",
        "            optimizee.zero_grad()\n",
        "            derivative_input = detach_var(derivative_input2)\n",
        "        else:\n",
        "            for name, p in optimizee.all_named_parameters():\n",
        "                rsetattr(optimizee, name, result_params[name])\n",
        "            assert len(list(optimizee.all_named_parameters()))\n",
        "            derivative_input = derivative_input2\n",
        "\n",
        "    return all_losses_ever\n",
        "\n",
        "@cache.cache\n",
        "def fit_optimizer_HNN(target_cls, target_to_opt, preproc=False, unroll=1, optim_it=1, n_epochs=20, n_tests=100, lr=0.001, out_mul=1.0):\n",
        "    opt_net = w(Optimizer_HNN(preproc=preproc))\n",
        "    meta_opt = optim.Adam(opt_net.parameters(), lr=lr, weight_decay=3e-6)\n",
        "    scheduler = StepLR(meta_opt, step_size=1, gamma=0.9)\n",
        "    best_net = None\n",
        "    best_loss = 100000000000000000\n",
        "    \n",
        "    for _ in tqdm(range(n_epochs), 'epochs'):\n",
        "        for _ in tqdm(range(20), 'iterations'):\n",
        "            do_fit_HNN(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=True)\n",
        "        \n",
        "        loss = (np.mean([\n",
        "            np.mean(do_fit_HNN(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=False))\n",
        "            for _ in tqdm(range(n_tests), 'tests')\n",
        "        ]))\n",
        "        print(loss)\n",
        "        if loss < best_loss:\n",
        "            print(best_loss, loss)\n",
        "            best_loss = loss\n",
        "            best_net = copy.deepcopy(opt_net.state_dict())\n",
        "        scheduler.step() \n",
        "    return best_loss, best_net"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1LDsT4Wnma3"
      },
      "source": [
        "@cache.cache\n",
        "def fit_normal(target_cls, target_to_opt, opt_class, n_tests=100, n_epochs=100, **kwargs):\n",
        "    results = []\n",
        "    for i in tqdm(range(n_tests), 'tests'):\n",
        "        target = target_cls(training=False)\n",
        "        optimizee = w(target_to_opt())\n",
        "        optimizer = opt_class(optimizee.parameters(), **kwargs)\n",
        "        total_loss = []\n",
        "        for _ in range(n_epochs):\n",
        "            loss = optimizee(target)\n",
        "            \n",
        "            total_loss.append(loss.data.cpu().numpy())\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        results.append(total_loss)\n",
        "    return results\n",
        "\n",
        "def find_best_lr_normal(target_cls, target_to_opt, opt_class, **extra_kwargs):\n",
        "    best_loss = 1000000000000000.0\n",
        "    best_lr = 0.0\n",
        "    for lr in tqdm([1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001, 0.00003, 0.00001], 'Learning rates'):\n",
        "        try:\n",
        "            loss = best_loss + 1.0\n",
        "            loss = np.mean([np.sum(s) for s in fit_normal(target_cls, target_to_opt, opt_class, lr=lr, **extra_kwargs)])\n",
        "        except RuntimeError:\n",
        "            pass\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_lr = lr\n",
        "    return best_loss, best_lrModuleDict"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJRfaW0ZreSf"
      },
      "source": [
        "class Optimizer(nn.Module):\n",
        "    def __init__(self, preproc=False, hidden_sz=20, preproc_factor=10.0):\n",
        "        super().__init__()\n",
        "        self.hidden_sz = hidden_sz\n",
        "        if preproc:\n",
        "            self.recurs = nn.LSTMCell(2, hidden_sz)\n",
        "        else:\n",
        "            self.recurs = nn.LSTMCell(1, hidden_sz)\n",
        "        self.recurs2 = nn.LSTMCell(hidden_sz, hidden_sz)\n",
        "        self.output = nn.Linear(hidden_sz, 1)\n",
        "        self.preproc = preproc\n",
        "        self.preproc_factor = preproc_factor\n",
        "        self.preproc_threshold = np.exp(-preproc_factor)\n",
        "        \n",
        "    def forward(self, inp, hidden, cell):\n",
        "        if self.preproc:\n",
        "            # Implement preproc described in Appendix A\n",
        "            \n",
        "            # Note: we do all this work on tensors, which means\n",
        "            # the gradients won't propagate through inp. This\n",
        "            # should be ok because the algorithm involves\n",
        "            # making sure that inp is already detached.\n",
        "            inp = inp.data\n",
        "            inp2 = w(torch.zeros(inp.size()[0], 2))\n",
        "            keep_grads = (torch.abs(inp) >= self.preproc_threshold).squeeze()\n",
        "            inp2[:, 0][keep_grads] = (torch.log(torch.abs(inp[keep_grads]) + 1e-8) / self.preproc_factor).squeeze()\n",
        "            inp2[:, 1][keep_grads] = torch.sign(inp[keep_grads]).squeeze()\n",
        "            \n",
        "            inp2[:, 0][~keep_grads] = -1\n",
        "            inp2[:, 1][~keep_grads] = (float(np.exp(self.preproc_factor)) * inp[~keep_grads]).squeeze()\n",
        "            inp = w(Variable(inp2))\n",
        "        hidden0, cell0 = self.recurs(inp, (hidden[0], cell[0]))\n",
        "        hidden1, cell1 = self.recurs2(hidden0, (hidden[1], cell[1]))\n",
        "        return self.output(hidden1), (hidden0, hidden1), (cell0, cell1)\n",
        "\n",
        "class Optimizer_HNN(nn.Module):\n",
        "    def __init__(self, preproc=False, preproc_factor=10.0):\n",
        "        super().__init__()\n",
        "        if preproc:\n",
        "            gdefunc = Hamiltonian(2)\n",
        "            self.output = nn.Linear(4, 1)\n",
        "        else:\n",
        "            gdefunc = Hamiltonian(1)\n",
        "            self.output = nn.Linear(2, 1)\n",
        "\n",
        "        #self.gde = ODEBlock(odefunc=gdefunc)\n",
        "        self.gde = gdefunc\n",
        "        self.preproc = preproc\n",
        "        self.preproc_factor = preproc_factor\n",
        "        self.preproc_threshold = np.exp(-preproc_factor)\n",
        "        \n",
        "    def forward(self, inp):\n",
        "        if self.preproc:\n",
        "            # Implement preproc described in Appendix A\n",
        "            \n",
        "            # Note: we do all this work on tensors, which means\n",
        "            # the gradients won't propagate through inp. This\n",
        "            # should be ok because the algorithm involves\n",
        "            # making sure that inp is already detached.\n",
        "            inp = inp.data\n",
        "            inp2 = w(torch.zeros(inp.size()[0], 2))\n",
        "            keep_grads = (torch.abs(inp) >= self.preproc_threshold).squeeze()\n",
        "            inp2[:, 0][keep_grads] = (torch.log(torch.abs(inp[keep_grads]) + 1e-8) / self.preproc_factor).squeeze()\n",
        "            inp2[:, 1][keep_grads] = torch.sign(inp[keep_grads]).squeeze()\n",
        "            \n",
        "            inp2[:, 0][~keep_grads] = -1\n",
        "            inp2[:, 1][~keep_grads] = (float(np.exp(self.preproc_factor)) * inp[~keep_grads]).squeeze()\n",
        "            inp = w(Variable(inp2))\n",
        "        #h = self.gde.trajectory(inp, T=1, num_points=20)[10]\n",
        "        h = self.gde(t=1, x=inp)\n",
        "        return self.output(h)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xvIY7BMnFGr"
      },
      "source": [
        "# Quadratic loss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q19RVZqTmv1O"
      },
      "source": [
        "class QuadraticLoss:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.W = w(Variable(torch.randn(10, 10)))\n",
        "        self.y = w(Variable(torch.randn(10)))\n",
        "        \n",
        "    def get_loss(self, theta):\n",
        "        return torch.sum((self.W.matmul(theta) - self.y)**2)\n",
        "    \n",
        "class QuadOptimizee(MetaModule):\n",
        "    def __init__(self, theta=None):\n",
        "        super().__init__()\n",
        "        self.register_buffer('theta', to_var(torch.zeros(10).cuda(), requires_grad=True))\n",
        "        \n",
        "    def forward(self, target):\n",
        "        return target.get_loss(self.theta)\n",
        "    \n",
        "    def all_named_parameters(self):\n",
        "        return [('theta', self.theta)]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jem4kRgZ6GMo"
      },
      "source": [
        "loss_LSTM, quad_optimizer_LSTM = fit_optimizer(QuadraticLoss, QuadOptimizee, unroll=20, optim_it=100, lr=0.001, n_tests=10, n_epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a3MHHx6fUR9"
      },
      "source": [
        "loss_HNN, quad_optimizer_HNN = fit_optimizer_HNN(QuadraticLoss, QuadOptimizee, unroll=20, optim_it=100, lr=0.001, n_tests=10, n_epochs=100, out_mul=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMzVVKGsx_fC",
        "outputId": "1bd41e60-6b5a-4b72-993a-f48c11b1a0de"
      },
      "source": [
        "print('Best loss of LSTM = ', loss_LSTM)\n",
        "print('Best loss of HNN = ', loss_HNN)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best loss of LSTM =  0.43030626\n",
            "Best loss of HNN =  0.77624464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7kD4_aawzkM"
      },
      "source": [
        "fit_data = np.zeros((100, 100, 2))\n",
        "np.random.seed(0)\n",
        "\n",
        "opt = w(Optimizer())\n",
        "opt.load_state_dict(quad_optimizer_LSTM)\n",
        "fit_data[:, :, 0] = np.array([do_fit(opt, None, QuadraticLoss, QuadOptimizee, 1, 100, 100, out_mul=1.0, should_train=False) for _ in range(100)])\n",
        "opt = w(Optimizer_HNN())\n",
        "opt.load_state_dict(quad_optimizer_HNN)\n",
        "fit_data[:, :, 1] = np.array([do_fit_HNN(opt, None, QuadraticLoss, QuadOptimizee, 1, 100, 100, out_mul=0.001, should_train=False) for _ in range(100)])"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXMsTtjjnIrO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "c1e94000-0cd7-4b36-8886-40237363dad4"
      },
      "source": [
        "plt.plot(np.mean(fit_data[:,:,0], axis=0), color='b', label='LSTM')\n",
        "plt.plot(np.mean(fit_data[:,:,1], axis=0), color='r', label='HNN')\n",
        "plt.xlabel('steps')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Quadratic functions')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNBnq9BKjqeB"
      },
      "source": [
        "# MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkGdgsLLjsg0"
      },
      "source": [
        "class MNISTLoss:\n",
        "    def __init__(self, training=True):\n",
        "        dataset = datasets.MNIST(\n",
        "            '/home/chenwy/mnist', train=True, download=True,\n",
        "            transform=torchvision.transforms.ToTensor()\n",
        "        )\n",
        "        indices = list(range(len(dataset)))\n",
        "        np.random.RandomState(10).shuffle(indices)\n",
        "        if training:\n",
        "            indices = indices[:len(indices) // 2]\n",
        "        else:\n",
        "            indices = indices[len(indices) // 2:]\n",
        "\n",
        "        self.loader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_size=128,\n",
        "            sampler=torch.utils.data.sampler.SubsetRandomSampler(indices))\n",
        "\n",
        "        self.batches = []\n",
        "        self.cur_batch = 0\n",
        "        \n",
        "    def sample(self):\n",
        "        if self.cur_batch >= len(self.batches):\n",
        "            self.batches = []\n",
        "            self.cur_batch = 0\n",
        "            for b in self.loader:\n",
        "                self.batches.append(b)\n",
        "        batch = self.batches[self.cur_batch]\n",
        "        self.cur_batch += 1\n",
        "        return batch\n",
        "\n",
        "class MNISTNet(MetaModule):\n",
        "    def __init__(self, layer_size=20, n_layers=1, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        inp_size = 28*28\n",
        "        self.layers = {}\n",
        "        for i in range(n_layers):\n",
        "            self.layers[f'mat_{i}'] = MetaLinear(inp_size, layer_size)\n",
        "            inp_size = layer_size\n",
        "\n",
        "        self.layers['final_mat'] = MetaLinear(inp_size, 10)\n",
        "        self.layers = nn.ModuleDict(self.layers)\n",
        "\n",
        "        self.activation = nn.Sigmoid()\n",
        "        self.loss = nn.NLLLoss()\n",
        "\n",
        "    def all_named_parameters(self):\n",
        "        return [(k, v) for k, v in self.named_parameters()]\n",
        "    \n",
        "    def forward(self, loss):\n",
        "        inp, out = loss.sample()\n",
        "        inp = w(Variable(inp.view(inp.size()[0], 28*28)))\n",
        "        out = w(Variable(out))\n",
        "\n",
        "        cur_layer = 0\n",
        "        while f'mat_{cur_layer}' in self.layers:\n",
        "            inp = self.activation(self.layers[f'mat_{cur_layer}'](inp))\n",
        "            cur_layer += 1\n",
        "\n",
        "        inp = F.log_softmax(self.layers['final_mat'](inp), dim=1)\n",
        "        l = self.loss(inp, out)\n",
        "        return l"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FpuRNnTFM-B"
      },
      "source": [
        "loss_LSTM, MNIST_optimizer_LSTM = fit_optimizer(MNISTLoss, MNISTNet, unroll=20, optim_it=100, lr=0.001, out_mul=0.1, preproc=True, n_tests=5, n_epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s767l7KFM-C"
      },
      "source": [
        "loss_HNN, MNIST_optimizer_HNN = fit_optimizer_HNN(MNISTLoss, MNISTNet, unroll=1, optim_it=1, lr=0.001, out_mul=0.1, n_tests=5, n_epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCH72JO-FM-D",
        "outputId": "62575091-0f0b-4207-8429-fb2b4ec15d32"
      },
      "source": [
        "print('Best loss of LSTM = ', loss_LSTM)\n",
        "print('Best loss of HNN = ', loss_HNN)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best loss of LSTM =  2.3219166\n",
            "Best loss of HNN =  2.3425555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7z-GGpLFM-E"
      },
      "source": [
        "fit_data = np.zeros((100, 100, 2))\n",
        "np.random.seed(0)\n",
        "\n",
        "opt = w(Optimizer(preproc=True))\n",
        "opt.load_state_dict(MNIST_optimizer_LSTM)\n",
        "fit_data[:, :, 0] = np.array([do_fit(opt, None, MNISTLoss, MNISTNet, 1, 100, 200, out_mul=0.1, should_train=False) for _ in range(100)])\n",
        "opt = w(Optimizer_HNN())\n",
        "opt.load_state_dict(MNIST_optimizer_HNN)\n",
        "fit_data[:, :, 1] = np.array([do_fit_HNN(opt, None, MNISTLoss, MNISTNet, 1, 100, 200, out_mul=0.001, should_train=False) for _ in range(100)])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnVuS2jIFM-F"
      },
      "source": [
        "plt.plot(np.mean(fit_data[:,:,0], axis=0), color='b', label='LSTM')\n",
        "plt.plot(np.mean(fit_data[:,:,1], axis=0), color='r', label='HNN')\n",
        "plt.xlabel('steps')\n",
        "plt.ylabel('loss')\n",
        "plt.title('MNIST')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}